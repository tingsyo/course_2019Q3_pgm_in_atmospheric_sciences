{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SISR with Plain DNN\n",
    "\n",
    "There are amny ways to construct a deep neural network. The following figure illustrate the structures of VGG-19, AlexNet, and ResNet:\n",
    "\n",
    "<img src='images/resnet.png' />\n",
    "\n",
    "In this example, we test SISR with a plain-convolutional-neural-network (VGG-like)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, logging, argparse, glob, h5py, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanAbsoluteError, MeanSquaredError\n",
    "#-----------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "#-----------------------------------------------------------------------\n",
    "# Load input/output data for model\n",
    "def loadIOTab(srcx, srcy, dropna=False):\n",
    "    import pandas as pd\n",
    "    # Scan for input data\n",
    "    logging.info(\"Reading input X from: \"+ srcx)\n",
    "    xfiles = []\n",
    "    for root, dirs, files in os.walk(srcx): \n",
    "        for fn in files: \n",
    "            if fn.endswith('.npy'): \n",
    "                 xfiles.append({'date':fn.replace('.npy',''), 'xuri':os.path.join(root, fn)})\n",
    "    xfiles = pd.DataFrame(xfiles)\n",
    "    print(\"... read input size: \"+str(xfiles.shape))\n",
    "    # Scan for input data\n",
    "    logging.info(\"Reading output Y from: \"+ srcy)\n",
    "    yfiles = []\n",
    "    for root, dirs, files in os.walk(srcy): \n",
    "        for fn in files: \n",
    "            if fn.endswith('.npy'): \n",
    "                 yfiles.append({'date':fn.replace('.npy',''), 'yuri':os.path.join(root, fn)})\n",
    "    yfiles = pd.DataFrame(yfiles)\n",
    "    print(\"... read output size: \"+str(yfiles.shape))\n",
    "    # Create complete IO-data\n",
    "    print(\"Merge input/output data.\")\n",
    "    iotab = pd.merge(yfiles, xfiles, on='date', sort=True)\n",
    "    print(\"... data size after merging: \"+str(iotab.shape))\n",
    "    # Done\n",
    "    return(iotab)\n",
    "\n",
    "def load_sprec(flist):\n",
    "    ''' Load a list a Surface Precipitation files (in npy format) into one numpy array. '''\n",
    "    xdata = []\n",
    "    for f in flist:\n",
    "        tmp = np.load(f)                            # Load numpy array\n",
    "        xdata.append(np.expand_dims(tmp, axis=2))   # Append axis to the end\n",
    "    x = np.array(xdata, dtype=np.float32)\n",
    "    return(x)\n",
    "\n",
    "def data_generator(iotab, batch_size):\n",
    "    ''' Data generator for batched processing. '''\n",
    "    nSample = len(iotab)\n",
    "    # This line is just to make the generator infinite, keras needs that\n",
    "    while True:\n",
    "        # Initialize the sample counter\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < nSample:\n",
    "            limit = min(batch_end, nSample)                     # Correct the end-pointer for the final batch\n",
    "            X = load_sprec(iotab['xuri'][batch_start:limit])    # Load X\n",
    "            Y = load_sprec(iotab['yuri'][batch_start:limit])    # Load Y\n",
    "            yield (X,Y)                                         # Send a tuple with two numpy arrays\n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "    # End of generator\n",
    "\n",
    "# Function to give report\n",
    "def report_sisr(y_true, y_pred):\n",
    "    import pandas as pd\n",
    "    import sklearn.metrics as metrics\n",
    "    output = []\n",
    "    # Loop through samples\n",
    "    n, h, w, c = y_true.shape\n",
    "    for i in range(n):\n",
    "        yt = y_true[i,:,:,:].flatten()\n",
    "        yp = y_pred[i,:,:,:].flatten()\n",
    "        # Calculate measures\n",
    "        results = {}\n",
    "        results['y_true_mean'] = yt.mean()\n",
    "        results['y_true_var'] = yt.var()\n",
    "        results['y_pred_mean'] = yp.mean()\n",
    "        results['y_pred_var'] = yp.var()\n",
    "        results['rmse'] = np.sqrt(metrics.mean_squared_error(yt,yp))\n",
    "        if y_pred.var()<=10e-8:\n",
    "            results['corr'] = 0\n",
    "        else:\n",
    "            results['corr'] = np.corrcoef(yt,yp)[0,1]\n",
    "        output.append(results)\n",
    "    # Return results\n",
    "    return(pd.DataFrame(output))\n",
    "\n",
    "# Create cross validation splits\n",
    "def create_splits(iotable, prop=0.2):\n",
    "    idxes = np.arange(iotable.shape[0])     # Create indexes\n",
    "    idxes = np.random.permutation(idxes)    # Permute indexes\n",
    "    idx_break = int(len(idxes)*prop)        # Index for the split point\n",
    "    idx_test = idxes[:idx_break]\n",
    "    idx_train = idxes[idx_break:]\n",
    "    return((idx_train, idx_test))\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "# The model\n",
    "#-----------------------------------------------------------------------\n",
    "def init_model_plaindnn(input_shape):\n",
    "    \"\"\"\n",
    "    :Return: \n",
    "      Newly initialized model for image up-scaling.\n",
    "    :param \n",
    "      int input_shape: The number of variables to use as input features.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # blovk1: CONV -> CONV\n",
    "    x = BatchNormalization(axis=-1)(inputs)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='conv1', padding='same')(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='conv2', padding='same')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='conv3', padding='same')(x)\n",
    "    # Output block: UP_SAMPLE -> CONV\n",
    "    x = UpSampling2D((2, 2), name='upsampple')(x)\n",
    "    x = Conv2D(filters=1, kernel_size=(3,3), activation='relu', name='conv4', padding='same')(x)\n",
    "    out = BatchNormalization(axis=-1)(x)\n",
    "    # Initialize model\n",
    "    model = Model(inputs = inputs, outputs = out)\n",
    "    # Define compile parameters\n",
    "    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mae', optimizer=adam, metrics=['mse','cosine_similarity'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "#DIRORI = 'D:\\\\data\\\\vvm_sprec\\\\original\\\\'\n",
    "#DIR2nd = 'D:\\\\data\\\\vvm_sprec\\\\scale_0.5\\\\'\n",
    "#DIR4th = 'D:\\\\data\\\\vvm_sprec\\\\scale_0.25\\\\'\n",
    "#DIR8th = 'D:\\\\data\\\\vvm_sprec\\\\scale_0.125\\\\'\n",
    "DIRORI = '/home/tsyo/sisrdata/original/'\n",
    "DIR2nd = '/home/tsyo/sisrdata/scale_0.5/'\n",
    "DIR4th = '/home/tsyo/sisrdata/scale_0.25/'\n",
    "DIR8th = '/home/tsyo/sisrdata/scale_0.125/'\n",
    "\n",
    "import numpy as np\n",
    "import logging, os\n",
    "import joblib\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading input X from: /home/tsyo/sisrdata/scale_0.125/\n",
      "INFO:root:Reading output Y from: /home/tsyo/sisrdata/scale_0.25/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... read input size: (2585, 2)\n",
      "... read output size: (2585, 2)\n",
      "Merge input/output data.\n",
      "... data size after merging: (2585, 3)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128, 128, 1)       4         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "upsampple (UpSampling2D)     (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 256, 256, 1)       577       \n",
      "=================================================================\n",
      "Total params: 75,333\n",
      "Trainable params: 75,203\n",
      "Non-trainable params: 130\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "iotab = loadIOTab(DIR8th, DIR4th, dropna=True)\n",
    "idx_trains, idx_tests = create_splits(iotab)\n",
    "model = init_model_plaindnn((128, 128, 1))\n",
    "model.summary()\n",
    "\n",
    "steps_train = np.ceil(len(idx_trains)/64)\n",
    "steps_test = np.ceil(len(idx_tests)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 394s 12s/step - loss: 0.0470 - mse: 0.7474 - cosine_similarity: 0.0016\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(data_generator(iotab.iloc[idx_trains,:], 64), steps_per_epoch=steps_train, epochs=1, max_queue_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.4251700116887083, 0.07193213505976666, 0.04817474693982237, 0.047290743312418344, 0.04804149404081662, 0.04825442931407656, 0.047824668973626666, 0.04770570989100108, 0.046991414111287724, 0.04702789577109671], 'mse': [1.4727378, 0.73468393, 0.7347262, 0.7347137, 0.73467815, 0.7346771, 0.734693, 0.73467344, 0.7347365, 0.73474807], 'cosine_similarity': [0.017761327, 0.08921894, 0.045139316, 0.046012778, 0.04570692, 0.05332372, 0.046766564, 0.07473871, 0.07242183, 0.073652536]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, json\n",
    "model.save_weights('data/plain_dnn_weights.h5')\n",
    "with open('data/plain_dnn_model.json', 'w') as f:\n",
    "    json.dump(model.to_json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_generator(data_generator(iotab.iloc[idx_tests,:], 64), steps=steps_test, max_queue_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 256, 256, 1)\n",
      "(517, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "y_true = load_sprec(iotab['yuri'].iloc[idx_tests])\n",
    "print(y_true.shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true_mean</th>\n",
       "      <th>y_true_var</th>\n",
       "      <th>y_pred_mean</th>\n",
       "      <th>y_pred_var</th>\n",
       "      <th>rmse</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.013276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.194439</td>\n",
       "      <td>2.504146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.594350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076589</td>\n",
       "      <td>1.409674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.189764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031227</td>\n",
       "      <td>0.316459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563413</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_true_mean  y_true_var  y_pred_mean  y_pred_var      rmse  corr\n",
       "0     0.008202    0.013276          0.0         0.0  0.115512     0\n",
       "1     0.194439    2.504146          0.0         0.0  1.594350     0\n",
       "2     0.000258    0.000066          0.0         0.0  0.008155     0\n",
       "3     0.076589    1.409674          0.0         0.0  1.189764     0\n",
       "4     0.031227    0.316459          0.0         0.0  0.563413     0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = report_sisr(y_true, y_pred)\n",
    "report.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
