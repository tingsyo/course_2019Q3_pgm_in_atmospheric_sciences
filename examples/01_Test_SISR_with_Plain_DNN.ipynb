{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SISR with Plain DNN\n",
    "\n",
    "There are amny ways to construct a deep neural network. The following figure illustrate the structures of VGG-19, AlexNet, and ResNet:\n",
    "\n",
    "<img src='images/resnet.png' />\n",
    "\n",
    "In this example, we test SISR with a plain-convolutional-neural-network (VGG-like)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, logging, argparse, glob, h5py, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanAbsoluteError, MeanSquaredError\n",
    "#-----------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "#-----------------------------------------------------------------------\n",
    "# Load input/output data for model\n",
    "def loadIOTab(srcx, srcy, dropna=False):\n",
    "    import pandas as pd\n",
    "    # Scan for input data\n",
    "    logging.info(\"Reading input X from: \"+ srcx)\n",
    "    xfiles = []\n",
    "    for root, dirs, files in os.walk(srcx): \n",
    "        for fn in files: \n",
    "            if fn.endswith('.npy'): \n",
    "                 xfiles.append({'date':fn.replace('.npy',''), 'xuri':os.path.join(root, fn)})\n",
    "    xfiles = pd.DataFrame(xfiles)\n",
    "    print(\"... read input size: \"+str(xfiles.shape))\n",
    "    # Scan for input data\n",
    "    logging.info(\"Reading output Y from: \"+ srcy)\n",
    "    yfiles = []\n",
    "    for root, dirs, files in os.walk(srcy): \n",
    "        for fn in files: \n",
    "            if fn.endswith('.npy'): \n",
    "                 yfiles.append({'date':fn.replace('.npy',''), 'yuri':os.path.join(root, fn)})\n",
    "    yfiles = pd.DataFrame(yfiles)\n",
    "    print(\"... read output size: \"+str(yfiles.shape))\n",
    "    # Create complete IO-data\n",
    "    print(\"Merge input/output data.\")\n",
    "    iotab = pd.merge(yfiles, xfiles, on='date', sort=True)\n",
    "    print(\"... data size after merging: \"+str(iotab.shape))\n",
    "    # Done\n",
    "    return(iotab)\n",
    "\n",
    "def load_sprec(flist):\n",
    "    ''' Load a list a Surface Precipitation files (in npy format) into one numpy array. '''\n",
    "    xdata = []\n",
    "    for f in flist:\n",
    "        tmp = np.load(f)\n",
    "        xdata.append(tmp)\n",
    "    x = np.array(xdata, dtype=np.float32)\n",
    "    return(x)\n",
    "\n",
    "def data_generator(iotab, batch_size):\n",
    "    ''' Data generator for batched processing. '''\n",
    "    nSample = len(iotab)\n",
    "    # This line is just to make the generator infinite, keras needs that\n",
    "    while True:\n",
    "        # Initialize the sample counter\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < nSample:\n",
    "            limit = min(batch_end, nSample)                     # Correct the end-pointer for the final batch\n",
    "            X = load_sprec(iotab['xuri'][batch_start:limit])    # Load X\n",
    "            Y = load_sprec(iotab['yuri'][batch_start:limit])    # Load Y\n",
    "            yield (X,Y)                                         # Send a tuple with two numpy arrays\n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "    # End of generator\n",
    "\n",
    "# Function to give report\n",
    "def report_sisr(y_true, y_pred):\n",
    "    import sklearn.metrics as metrics\n",
    "    # Calculate measures\n",
    "    results = {}\n",
    "    results['y_true_mean'] = y_true.mean()\n",
    "    results['y_true_var'] = y_true.var()\n",
    "    results['y_pred_mean'] = y_pred.mean()\n",
    "    results['y_pred_var'] = y_pred.var()\n",
    "    results['rmse'] = np.sqrt(metrics.mean_squared_error(y_true,y_pred))\n",
    "    if y_pred.var()<=10e-8:\n",
    "        results['corr'] = 0\n",
    "    else:\n",
    "        results['corr'] = np.corrcoef(y_true,y_pred)[0,1]\n",
    "    # Return results\n",
    "    return(results)\n",
    "\n",
    "# Create cross validation splits\n",
    "def create_splits(iotable, prop=0.2):\n",
    "    idxes = np.arange(iotable.shape[0])     # Create indexes\n",
    "    idxes = np.random.permutation(idxes)    # Permute indexes\n",
    "    idx_break = int(len(idxes)*prop)        # Index for the split point\n",
    "    idx_test = idxes[:idx_break]\n",
    "    idx_train = idxes[idx_break:]\n",
    "    return((idx_train, idx_test))\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "# The model\n",
    "#-----------------------------------------------------------------------\n",
    "def init_model_plaindnn(input_shape):\n",
    "    \"\"\"\n",
    "    :Return: \n",
    "      Newly initialized model for image up-scaling.\n",
    "    :param \n",
    "      int input_shape: The number of variables to use as input features.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # blovk1: CONV -> CONV\n",
    "    x = BatchNormalization(axis=1)(inputs)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='conv1', padding='same')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='conv2', padding='same')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='conv3', padding='same')(x)\n",
    "    # Output block: UP_SAMPLE -> CONV\n",
    "    x = UpSampling2D((2, 2), name='upsampple')(x)\n",
    "    x = Conv2D(filters=1, kernel_size=(3,3), activation='relu', name='conv4', padding='same')(x)\n",
    "    out = BatchNormalization()(x)\n",
    "    # Initialize model\n",
    "    model = Model(inputs = inputs, outputs = out)\n",
    "    # Define compile parameters\n",
    "    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mae', optimizer=adam, metrics=['mse','cosine_similarity'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "DIRORI = 'D:\\\\data\\\\vvm_sprec\\\\original\\\\'\n",
    "DIR2nd = 'D:\\\\data\\\\vvm_sprec\\\\scale_0.5\\\\'\n",
    "DIR4th = 'D:\\\\data\\\\vvm_sprec\\\\scale_0.25\\\\'\n",
    "DIR8th = 'D:\\\\data\\\\vvm_sprec\\\\scale_0.125\\\\'\n",
    "\n",
    "import numpy as np\n",
    "import logging, os\n",
    "import joblib\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1115 11:06:26.958111 30764 <ipython-input-18-a17d842dd66a>:17] Reading input X from: D:\\data\\vvm_sprec\\scale_0.125\\\n",
      "I1115 11:06:26.969115 30764 <ipython-input-18-a17d842dd66a>:26] Reading output Y from: D:\\data\\vvm_sprec\\scale_0.25\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... read input size: (2585, 2)\n",
      "... read output size: (2585, 2)\n",
      "Merge input/output data.\n",
      "... data size after merging: (2585, 3)\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128, 128, 1)       512       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128, 128, 64)      512       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "upsampple (UpSampling2D)     (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 256, 256, 1)       577       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 256, 256, 1)       4         \n",
      "=================================================================\n",
      "Total params: 76,101\n",
      "Trainable params: 75,587\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "iotab = loadIOTab(DIR8th, DIR4th, dropna=True)\n",
    "idx_trains, idx_tests = create_splits(iotab)\n",
    "model = init_model_plaindnn((128, 128, 1))\n",
    "model.summary()\n",
    "\n",
    "steps_train = np.ceil(len(idx_trains)/64)\n",
    "steps_test = np.ceil(len(idx_tests)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_6 to have 4 dimensions, but got array with shape (64, 128, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-11d3287e4313>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miotab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_trains\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[0;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[0;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m   \u001b[1;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    563\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    566\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_6 to have 4 dimensions, but got array with shape (64, 128, 128)"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(data_generator(iotab.iloc[idx_trains,:], 64), steps_per_epoch=steps_train, epochs=10, max_queue_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
