{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models with Neural Networks\n",
    "\n",
    "- title: Generative Models with Neural Networks\n",
    "- author: Ting S. Yo\n",
    "- date: 2019.12.20\n",
    "\n",
    "In this notebook we want to introduce the concept of [**Generative Model**](https://en.wikipedia.org/wiki/Generative_model), and explore the feasibility and ways to embed **domain knowledge** into machine learning. We will use [Variational Autoencoders (VAE)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) and [Generative Adversarial Network (GAN)](https://en.wikipedia.org/wiki/Generative_adversarial_network) as example emthods.\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [A Beginner's Guide to Generative Adversarial Networks (GANs)](https://pathmind.com/wiki/generative-adversarial-network-gan)\n",
    "- [Understanding Variational Autoencoders (VAEs)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model\n",
    "\n",
    "In statistical learning, there are two main approaches for classification, **generative approach** and the **discriminative approach**. Terminology is inconsistent across many studies, but three major types can be distinguished, following Jebara (2004):\n",
    "\n",
    "- Given an observable variable X and a target variable Y, a *generative model is a statistical model of the joint probability distribution on X Ã— Y*, ${\\displaystyle P(X,Y)}$;\n",
    "\n",
    "- A discriminative model is a *model of the conditional probability of the target Y, given an observation x*, symbolically, ${\\displaystyle P(Y|X=x)}$; and\n",
    "\n",
    "- Classifiers computed without using a probability model are also referred to loosely as \"discriminative\".\n",
    "\n",
    "The distinction between these last two classes is not consistently made. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\n",
    "\n",
    "\n",
    "An alternative division defines these symmetrically as:\n",
    "\n",
    "- a generative model is a model of the conditional probability of the observable X, given a target y, symbolically, ${\\displaystyle P(X|Y=y)}$\n",
    "- a discriminative model is a model of the conditional probability of the target Y, given an observation x, symbolically, ${\\displaystyle P(Y|X=x)}$\n",
    "\n",
    "Regardless of precise definition, the terminology is constitutional because a generative model can be used to \"generate\" random instances (outcomes), either of an observation and target ${\\displaystyle (x,y)}$, or of an observation x given a target value y, while a discriminative model or discriminative classifier (without a model) can be used to \"discriminate\" the value of the target variable Y, given an observation x. The difference between \"discriminate\" (distinguish) and \"classify\" is subtle, and these are not consistently distinguished. (The term \"discriminative classifier\" becomes a pleonasm when \"discrimination\" is equivalent to \"classification\".)\n",
    "\n",
    "The term \"generative model\" is also used to describe *models that generate instances of output variables in a way that has no clear relationship to probability distributions over potential samples of input variables*. *Generative adversarial networks (GANs)* are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. Such models are not classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder (VAE)\n",
    "\n",
    "Variational autoencoders are a slightly more modern and interesting take on autoencoding.\n",
    "\n",
    "What is a variational autoencoder, you ask? It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\".\n",
    "\n",
    "How does a variational autoencoder work?\n",
    "\n",
    "First, an encoder network turns the input samples x into two parameters in a latent space, which we will note z_mean and z_log_sigma. Then, we randomly sample similar points z from the latent normal distribution that is assumed to generate the data, via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\n",
    "\n",
    "The parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
